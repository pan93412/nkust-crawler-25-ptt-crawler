import asyncio
import httpx
from datetime import datetime
from typing import final
import logging

from .models import Article, SearchResult
from .scrapers import ArticleScraper, CommentScraper, PaginationScraper
from .database import DatabaseManager, DatabaseStats

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Constants
BASE_URL = "https://www.ptt.cc"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36',
}
COOKIES = {
    'over18': '1'
}


@final
class PTTCrawler:
    """PTT çˆ¬èŸ²ä¸»é¡"""
    
    def __init__(self, db_path: str = "ptt_scraper.db", cutoff_days: int = 5):
        """
        åˆå§‹åŒ– PTT çˆ¬èŸ²
        
        Args:
            db_path: è³‡æ–™åº«æª”æ¡ˆè·¯å¾‘
            cutoff_days: åªæŠ“å–å¹¾å¤©å…§çš„æ–‡ç« ï¼ˆå¾ä»Šå¤©å¾€å‰ç®—ï¼‰
        """
        self.db_manager = DatabaseManager(db_path)
        self.cutoff_days = cutoff_days
        self.client: httpx.AsyncClient | None = None
        
    async def __aenter__(self):
        """éåŒæ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€²å…¥"""
        self.client = httpx.AsyncClient(timeout=30.0)
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """éåŒæ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        if self.client:
            await self.client.aclose()
    
    def get_cutoff_date(self) -> datetime:
        """å–å¾—æˆªæ­¢æ—¥æœŸ"""
        today = datetime.today()
        return today.replace(day=today.day - self.cutoff_days, hour=0, minute=0, second=0, microsecond=0)
    
    async def fetch_html(self, url: str) -> str:
        """
        å–å¾—ç¶²é  HTML å…§å®¹
        
        Args:
            url: ç›®æ¨™URL
            
        Returns:
            HTML å…§å®¹
        """
        if not self.client:
            raise RuntimeError("Client not initialized. Use 'async with' statement.")
            
        try:
            response = await self.client.get(url, headers=HEADERS, cookies=COOKIES)
            _ = response.raise_for_status()
            return response.text
        except Exception as e:
            logger.error(f"Error fetching HTML from {url}: {e}")
            return ""
    
    async def search_articles(self, board: str, keyword: str) -> list[SearchResult]:
        """
        æœå°‹æŒ‡å®šçœ‹æ¿çš„æ–‡ç« 
        
        Args:
            board: çœ‹æ¿åç¨±
            keyword: æœå°‹é—œéµå­—
            
        Returns:
            æœå°‹çµæœåˆ—è¡¨
        """
        search_url = f"{BASE_URL}/bbs/{board}/search?q={keyword}"
        cutoff_date = self.get_cutoff_date()
        logger.info(f"ğŸ“† Only crawling articles from {cutoff_date.strftime('%Y/%m/%d')}")
        
        all_results = list[SearchResult]()
        next_url = search_url
        
        while next_url:
            try:
                html_content = await self.fetch_html(next_url)
                if not html_content:
                    break
                    
                pagination_scraper = PaginationScraper(html_content)
                results, stop_flag = self._extract_search_results_from_page(
                    html_content, cutoff_date
                )
                
                all_results.extend(results)
                
                if stop_flag:
                    break
                
                # å–å¾—ä¸‹ä¸€é URLï¼ˆæ³¨æ„PTTçš„åˆ†é é †åºï¼‰
                pagination_info = pagination_scraper.get_pagination_info()
                if pagination_info.has_next:
                    next_url = pagination_info.next_page_url
                else:
                    break
                    
            except Exception as e:
                logger.error(f"Error searching articles: {e}")
                break
        
        logger.info(f"âœ… Found {len(all_results)} articles")
        return all_results
    
    def _extract_search_results_from_page(self, html_content: str, cutoff_date: datetime) -> tuple[list[SearchResult], bool]:
        """
        å¾æœå°‹çµæœé é¢æå–æ–‡ç« åˆ—è¡¨
        
        Args:
            html_content: é é¢HTMLå…§å®¹
            cutoff_date: æˆªæ­¢æ—¥æœŸ
            
        Returns:
            (æœå°‹çµæœåˆ—è¡¨, æ˜¯å¦æ‡‰è©²åœæ­¢æœå°‹)
        """
        from bs4 import BeautifulSoup
        
        results = list[SearchResult]()
        soup = BeautifulSoup(html_content, 'html.parser')
        r_ents = soup.select('div.r-ent')
        stop_flag = False
        
        for r in r_ents:
            date_element = r.select_one('.date')
            if not date_element:
                continue
                
            date_str = date_element.get_text(strip=True)
            link_element = r.select_one('div.title a')
            
            if not link_element:
                continue
            
            try:
                this_year = datetime.today().year
                full_date = datetime.strptime(f"{this_year}/{date_str}", "%Y/%m/%d")
            except ValueError:
                continue
            
            if full_date < cutoff_date:
                logger.info(f"â›” Stopping: {full_date.strftime('%Y/%m/%d')} is earlier than {cutoff_date.strftime('%Y/%m/%d')}")
                stop_flag = True
                break
            
            title = link_element.get_text(strip=True)
            href = link_element.get('href')
            
            if not isinstance(href, str):
                continue
                
            full_url = BASE_URL + href
            article_id = href.split('/')[-1].strip().replace('.html', '')
            
            logger.info(f"ğŸ“„ {full_date.strftime('%m/%d')} {title} -> {full_url}")
            
            results.append(SearchResult(
                id=article_id,
                title=title,
                url=full_url,
                created_at=full_date
            ))
        
        return results, stop_flag
    
    async def scrape_article(self, url: str) -> Article | None:
        """
        æŠ“å–å–®ä¸€æ–‡ç« åŠå…¶ç•™è¨€
        
        Args:
            url: æ–‡ç« URL
            
        Returns:
            Article ç‰©ä»¶æˆ– None
        """
        try:
            html_content = await self.fetch_html(url)
            if not html_content:
                return None
            
            # æå–æ–‡ç« è³‡è¨Š
            article_scraper = ArticleScraper(html_content)
            
            article_id = url.split('/')[-1].strip().replace('.html', '')
            title = article_scraper.extract_title()
            author = article_scraper.extract_author()
            content = article_scraper.extract_content()
            created_at = article_scraper.extract_datetime()
            
            # æå–ç•™è¨€
            comment_scraper = CommentScraper()
            soup = article_scraper.get_soup()
            comments = comment_scraper.extract_comments(soup, created_at.year)
            
            return Article(
                id=article_id,
                title=title,
                url=url,
                author=author,
                content=content,
                created_at=created_at,
                comments=comments
            )
            
        except Exception as e:
            logger.error(f"Error scraping article {url}: {e}")
            return None
    
    async def crawl_board(self, board: str, keyword: str, max_articles: int | None = None) -> int:
        """
        çˆ¬å–æŒ‡å®šçœ‹æ¿çš„æ–‡ç« 
        
        Args:
            board: çœ‹æ¿åç¨±
            keyword: æœå°‹é—œéµå­—
            max_articles: æœ€å¤§æ–‡ç« æ•¸é‡é™åˆ¶
            
        Returns:
            æˆåŠŸè™•ç†çš„æ–‡ç« æ•¸é‡
        """
        # æœå°‹æ–‡ç« 
        search_results = await self.search_articles(board, keyword)
        
        if max_articles:
            search_results = search_results[:max_articles]
        
        # å»ºç«‹ semaphore é™åˆ¶ä¸¦ç™¼æ•¸é‡
        semaphore = asyncio.Semaphore(4)
        processed_count = 0
        
        async def process_article(search_result: SearchResult) -> bool:
            nonlocal processed_count
            async with semaphore:
                logger.info(f"ğŸ” Processing: {search_result.title}")
                
                article = await self.scrape_article(search_result.url)
                if article:
                    try:
                        _ = self.db_manager.save_article(article)
                        logger.info(f"âœ… Saved: {article.title}")
                        processed_count += 1
                        return True
                    except Exception as e:
                        logger.error(f"âŒ Failed to save {article.title}: {e}")
                        return False
                else:
                    logger.warning(f"âš ï¸ Failed to scrape: {search_result.title}")
                    return False
        
        # ä¸¦ç™¼è™•ç†æ‰€æœ‰æ–‡ç« 
        tasks = [process_article(result) for result in search_results]
        _ = await asyncio.gather(*tasks, return_exceptions=True)
        
        logger.info(f"âœ… Crawling completed. Processed {processed_count} articles.")
        return processed_count
    
    def get_stats(self) -> DatabaseStats:
        """
        å–å¾—è³‡æ–™åº«çµ±è¨ˆè³‡è¨Š
        
        Returns:
            çµ±è¨ˆè³‡è¨Šå­—å…¸
        """
        return self.db_manager.get_database_stats()
    
    def close(self) -> None:
        """é—œé–‰è³‡æ–™åº«é€£ç·š"""
        self.db_manager.close()


async def crawl_ptt(board: str, keyword: str, db_path: str = "ptt_scraper.db", 
                   cutoff_days: int = 5, max_articles: int | None = None) -> int:
    """
    ä¾¿åˆ©å‡½æ•¸ï¼šçˆ¬å– PTT æ–‡ç« 
    
    Args:
        board: çœ‹æ¿åç¨±
        keyword: æœå°‹é—œéµå­—
        db_path: è³‡æ–™åº«æª”æ¡ˆè·¯å¾‘
        cutoff_days: åªæŠ“å–å¹¾å¤©å…§çš„æ–‡ç« 
        max_articles: æœ€å¤§æ–‡ç« æ•¸é‡é™åˆ¶
        
    Returns:
        æˆåŠŸè™•ç†çš„æ–‡ç« æ•¸é‡
    """
    async with PTTCrawler(db_path, cutoff_days) as crawler:
        return await crawler.crawl_board(board, keyword, max_articles) 